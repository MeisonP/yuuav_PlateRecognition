{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## label extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 2] No such file or directory: './Segment_Dataset/zh_char/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-40b0788cc6f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#reset image file name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mzh_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_zh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#all file/folder in path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzh_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mpath_zh_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_zh\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 2] No such file or directory: './Segment_Dataset/zh_char/'"
     ]
    }
   ],
   "source": [
    "#coding:utf-8\n",
    "#used for label extraction\n",
    "#2018.06.06\n",
    "#+—————zh_char:(path_zh)\n",
    "#+     |—————name=川，湘，...(path_zh_folder)\n",
    "#+           |—————file_=**.jpg....(dir)\n",
    "#+—————alphabet_number\n",
    "#############################################\n",
    "\n",
    "import os, sys, stat\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "path_zh='./Segment_Dataset/zh_char/'\n",
    "path_ch='./Segment_Dataset/ch_/'\n",
    "path_alphabet='./Segment_Dataset/alphabet_number/'\n",
    "\n",
    "#reset image file name\n",
    "zh_=os.listdir(path_zh)#all file/folder in path\n",
    "for name in np.array(zh_):\n",
    "    path_zh_folder=path_zh+name\n",
    "    #os.chmod(path_zh_folder,stat.S_IRWXU)\n",
    "    filelist=os.listdir(path_zh_folder)#image files name in zh_folder,image name+.jpg\n",
    "    i=0\n",
    "    for file_ in filelist:\n",
    "        olddir=os.path.join(path_zh_folder,file_)\n",
    "        filename=os.path.splitext(file_)[0]\n",
    "        filetype=os.path.splitext(file_)[1]\n",
    "        newdir=os.path.join(path_ch,name+str(i)+filetype)\n",
    "        os.rename(olddir,newdir)\n",
    "        i=i+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label2Txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "#write label into txt\n",
    "#2018.06.06 \n",
    "#############################################\n",
    "import os \n",
    "\n",
    "path_ch='./test_data/ch_/'\n",
    "path_number='./test_data/number_'\n",
    "\n",
    "with open (\"./test_data/test_label_ch.txt\",'a') as f:\n",
    "    filelist=os.listdir(path_ch)\n",
    "    for file_ in filelist:\n",
    "        filename=os.path.splitext(file_)[0]\n",
    "        label=filename[0:3]#eg one chinese char\n",
    "        filetype=os.path.splitext(file_)[1]\n",
    "        f.write(filename+':'+label+'\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## @Multiclass SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO -[:17]- **************************Mason(2018-06-12 17-42-23)*******************\n",
      "INFO -[:104]- feature extract and load data...\n",
      "INFO -[:111]- X_train shape:(17703,400)\n",
      "INFO -[:112]- Y_train(labels) shape:(17703,1)\n",
      "INFO -[:113]- time consuption of load data:35.5 second\n",
      "INFO -[:116]- create svm...\n",
      "INFO -[:125]- training...\n",
      "INFO -[:129]- time consuption of training:55.1085 second\n",
      "INFO -[:131]- save the trained svm...\n",
      "INFO -[:134]- Done!\n"
     ]
    }
   ],
   "source": [
    "#coding:utf-8\n",
    "#Creat an multiclass svm and train\n",
    "#2018.06.07\n",
    "#############################################\n",
    "import cv2 as cv \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os\n",
    "import logging\n",
    "import time, timeit\n",
    "\n",
    "def ini():  \n",
    "    TM=time.strftime(\"%Y-%m-%d %H-%M-%S\",time.localtime())\n",
    "    LOG_FORMAT = \"%(levelname)s -[:%(lineno)d]- %(message)s\"\n",
    "    #open(log_nm,'w').close()\n",
    "    logging.basicConfig(level=logging.INFO, format=LOG_FORMAT)\n",
    "    logging.info('**************************Mason(%s)*******************'%(TM,))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "#create dataset object and label object func\n",
    "def load_data(path_X,path_Y):#path_X is folder and path_Y is a txt file\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    filelist=os.listdir(path_X)\n",
    "    df=pd.read_table(path_Y,sep=\":\",header=None)#,encoding='utf-8'\n",
    "    df.columns=['name','label']\n",
    "    for file_ in filelist:\n",
    "        #read image\n",
    "        path_image=os.path.join(path_X,file_)\n",
    "        image=cv.imread(path_image,0)#must be gray model\n",
    "        #unisize\n",
    "        image=cv.resize(image,(20,20))\n",
    "        \n",
    "        feature=image\n",
    "        #image-->list\n",
    "        try:\n",
    "            N_pixels=(np.array(feature).shape[0])*(np.array(feature).shape[1])\n",
    "        except IndexError:\n",
    "            logging.error(\"image read error! feature shape is:\"%(np.array(feature).shape))\n",
    "            raise       \n",
    "        \n",
    "        image_vector=np.reshape(np.array(feature),N_pixels)\n",
    "        #logging.info('vector length of image %s: %d'%(file_,np.array(image_vector.shape)))\n",
    "        X.append(image_vector.tolist())\n",
    "        #index label\n",
    "        filename=os.path.splitext(file_)[0]\n",
    "        label=df.loc[df['name']==filename,'label']\n",
    "        Y.append(label.tolist())\n",
    "    return X,Y #X is dataset and Y is label related\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#train ch_ /alphabet_number\n",
    "if __name__=='__main__':\n",
    "    ini()\n",
    "    path_X_train='./Segment_Dataset/train_data/ch_'\n",
    "    path_Y_train='./Segment_Dataset/train_data/train_label_ch2.txt'\n",
    "    #get dataset and label object\n",
    "    logging.info('feature extract and load data...')\n",
    "    load_begin= timeit.default_timer()  \n",
    "    \n",
    "    X_train,Y_train=load_data(path_X_train,path_Y_train)\n",
    "    \n",
    "    #logging.info(np.array(Y_train).shape) \n",
    "    load_end = timeit.default_timer()  \n",
    "    logging.info('X_train shape:(%d,%d)'%(np.array(X_train).shape[0],np.array(X_train).shape[1]))\n",
    "    logging.info('Y_train(labels) shape:(%d,%d)'%(np.array(Y_train).shape[0],np.array(Y_train).shape[1]))\n",
    "    logging.info('time consuption of load data:%0.1f second'%(load_end-load_begin))\n",
    "    \n",
    "    #create svm object\n",
    "    logging.info('create svm...')\n",
    "    svm=cv.ml.SVM_create()\n",
    "    svm.setKernel(cv.ml.SVM_LINEAR)#cv.ml.SVM_LINEAR,\n",
    "    svm.setType(cv.ml.SVM_C_SVC)#cv.ml.SVM_EPS_SVR,  cv.ml.SVM_C_SVC\n",
    "    svm.setGamma(2.0)\n",
    "    svm.setC(5.0)\n",
    "    svm.setP(1.0)#must postive for SVM_EPS_SVR type\n",
    "    #svm.setNu(0.5)# only for SVM_NU_SVC type and between 0-1\n",
    "    #training &save svm\n",
    "    logging.info('training...')\n",
    "    train_begin= timeit.default_timer() \n",
    "    svm.train(np.array(X_train,dtype=np.float32),cv.ml.ROW_SAMPLE,np.array(Y_train))\n",
    "    load_end= timeit.default_timer() \n",
    "    logging.info('time consuption of training:%0.4f second'%(load_end-load_begin))\n",
    "    \n",
    "    logging.info('save the trained svm...')\n",
    "\n",
    "    svm.save('./svm_model.m')#‘path’\n",
    "\n",
    "    #predicate \n",
    "    logging.info('Done!')\n",
    "    #Y_test_predict=svm.predict(np.array(X_test,dtype=np.float32))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## @Single Image Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "粤\n"
     ]
    }
   ],
   "source": [
    "#coding:utf-8\n",
    "#load svm model and just for predict single image\n",
    "#############################################\n",
    "\n",
    "import cv2 as cv \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df=pd.read_table('./Segment_Dataset/mapping.txt',sep=\":\",header=None)#,encoding='utf-8'\n",
    "df.columns=['ch','nb']\n",
    "X_=[]\n",
    "zh=[]\n",
    "extract=0\n",
    "if extract==1:\n",
    "    svm=cv.ml.SVM_load('./new_svm_model.m')\n",
    "else:\n",
    "    svm=cv.ml.SVM_load('./svm_model.m')\n",
    "image=cv.imread('./Segment_Dataset/粤_.png',cv.IMREAD_GRAYSCALE)\n",
    "image=cv.resize(image,(20,20))\n",
    "\n",
    "feature=image\n",
    "N=(np.array(feature).shape[1])*(np.array(feature).shape[0])\n",
    "\n",
    "img=np.reshape(np.array(feature),N)\n",
    "\n",
    "X_.append(img.tolist())# X_train is a list; can not use extend\n",
    "Y_predict=svm.predict(np.array(X_,dtype=np.float32),cv.ml.ROW_SAMPLE)\n",
    "result=int(Y_predict[1][0][0])\n",
    "print result\n",
    "ch=df.loc[df['nb']==result,'ch']\n",
    "zh.append(ch.tolist())# an 2D array\n",
    "print np.array(zh)[0][0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA-Feature Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 20)\n",
      "shape of data is (20, 20)\n",
      "shape of eig_vector: (20, 20)\n",
      "shape of eig_val: (20,)\n",
      "[5.340923068585198e-14, 0.003794851206848404, 0.03317634372257493, 11.651331545912, 18.036645808691954, 34.98458149660998, 72.19024249791258, 142.69186185013484, 206.43260484830193, 377.85652431021055, 726.8297050405346]\n",
      "shape of selected eig_val: (11,)\n",
      "shape of selected eig_vector: (10, 20)\n",
      "shape of new_data: (10, 20)\n"
     ]
    }
   ],
   "source": [
    "#coding:utf-8\n",
    "#adding PCA calculation\n",
    "#2018/06/11\n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv \n",
    "\n",
    "image=cv.imread('./Segment_Dataset/粤_5.bmp',cv.IMREAD_GRAYSCALE)\n",
    "image=cv.resize(image,(20,20))\n",
    "mat_=np.array(image)#40*32\n",
    "print np.array(mat_).shape\n",
    "#Mean & Normalization \n",
    "Mean=[np.mean(mat_[:,i]) for i in range(0,mat_.shape[1])]\n",
    "Z=[np.array(mat_[:,i]-Mean[i]) for i in range(0,mat_.shape[1])]\n",
    "'''#equal to s\n",
    "Z=[]\n",
    "for i in range(0,mat_.shape[1]):# shape[0] is hight of image in numpy (h,w) \n",
    "    M=np.mean(mat_[:,i])# mean value of each column\n",
    "    Z.append(mat_[:,i]-M)# normalized '''\n",
    "    \n",
    "data=np.transpose(Z)\n",
    "#data=Z\n",
    "print 'shape of data is {}'.format(np.array(data).shape)\n",
    "# Covariance\n",
    "cov=np.cov(data)#32*32\n",
    "#print cov\n",
    "# calculating eigenvalue&vector\n",
    "eig_val, eig_vector = np.linalg.eig(cov)\n",
    "\n",
    "print 'shape of eig_vector: {}'.format(np.array(eig_vector).shape)\n",
    "print 'shape of eig_val: {}'.format(np.array(eig_val).shape)\n",
    "eig_pairs = [(np.abs(eig_val[i]), eig_vector[:,i]) for i in range(len(eig_val))]\n",
    "\n",
    "#select the top k eig_\n",
    "index_=np.argsort(eig_val)#totally 32 eig_pairs\n",
    "\n",
    "# select higher 11 eig\n",
    "eig_val_n=[np.abs(eig_val[i]) for i in index_[0:11:1]]\n",
    "print eig_val_n\n",
    "eig_vector_n=[(eig_vector[:,i]) for i in index_[0:10:1]]\n",
    "print 'shape of selected eig_val: {}'.format(np.array(eig_val_n).shape )\n",
    "print 'shape of selected eig_vector: {}'.format(np.array(eig_vector_n).shape )\n",
    "#reconstructs new data\n",
    "new_data=np.dot(eig_vector_n,data) # data*eigenvector 所谓降维，降的是h 的维度，也就是row 的数量\n",
    "print 'shape of new_data: {}'.format(np.array(new_data).shape )\n",
    "#print  new_data\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
